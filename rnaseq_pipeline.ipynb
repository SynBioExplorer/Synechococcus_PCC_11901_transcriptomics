{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RNA-seq Analysis Pipeline: *Picosynechococcus sp.* PCC 11901\n",
    "\n",
    "**Project**: Transcriptomic analysis of PCC 11901 under various nutrient, environmental, and circadian conditions\n",
    "\n",
    "**Experimental Design**:\n",
    "- 20 conditions × 3 biological replicates = 60 samples\n",
    "- Group 1: Nutrient conditions (Control: U4,5,6)\n",
    "- Group 2: Environmental conditions (Control: U1,2,3)\n",
    "- Group 3: Circadian rhythm (T1-T4 timepoints)\n",
    "\n",
    "**Pipeline Overview**:\n",
    "1. Quality Control (FastQC + MultiQC)\n",
    "2. Trimming (fastp)\n",
    "3. Lane Merging\n",
    "4. rRNA Removal (SortMeRNA)\n",
    "5. Quantification (Salmon)\n",
    "6. Alignment for Visualization (Bowtie2 → bigWig)\n",
    "7. Differential Expression (DESeq2 via rpy2)\n",
    "8. Visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "# === Standard Library ===\nimport os\nimport subprocess\nimport glob\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# === Fix for rpy2 on Apple Silicon ===\n# Force R_HOME to conda env's R before importing rpy2\n# This prevents conflicts with System R\nif 'CONDA_PREFIX' in os.environ:\n    os.environ['R_HOME'] = os.path.join(os.environ['CONDA_PREFIX'], 'lib', 'R')\n\n# === Data Science ===\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# === Visualization ===\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# === Progress & Parallelization ===\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed\n\n# === R Integration ===\nimport rpy2.robjects as ro\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects.conversion import localconverter\n\n# Activate pandas/R dataframe conversion\npandas2ri.activate()\n\n# Set plotting defaults\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('colorblind')\npd.set_option('display.max_columns', 50)\n\nprint(\"All packages imported successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Project Configuration ===\n",
    "\n",
    "# Base directory (adjust if needed)\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "\n",
    "# Directory structure\n",
    "DIRS = {\n",
    "    'data': BASE_DIR / 'Data',\n",
    "    'genome': BASE_DIR / 'PCC_11901_annotated genome',\n",
    "    'qc': BASE_DIR / '01_QC',\n",
    "    'trimmed': BASE_DIR / '02_trimmed',\n",
    "    'merged': BASE_DIR / '03_merged',\n",
    "    'rrna_filtered': BASE_DIR / '04_rRNA_filtered',\n",
    "    'salmon': BASE_DIR / '05_salmon',\n",
    "    'alignment': BASE_DIR / '06_alignment',\n",
    "    'deseq2': BASE_DIR / '07_deseq2',\n",
    "    'functional': BASE_DIR / '08_functional',\n",
    "    'figures': BASE_DIR / '09_figures',\n",
    "    'logs': BASE_DIR / 'logs',\n",
    "}\n",
    "\n",
    "# Reference files\n",
    "GENOME_FASTA = DIRS['genome'] / 'GCF_005577135.1_ASM557713v1_genomic.fna'\n",
    "GTF_FILE = DIRS['genome'] / 'genomic.gtf'\n",
    "TRANSCRIPTOME_FASTA = DIRS['genome'] / 'transcriptome.fa'  # Will be generated\n",
    "\n",
    "# Hardware settings (M2 Max optimized)\n",
    "N_THREADS = 8  # General parallelization\n",
    "SORTMERNA_THREADS = 4  # Memory-intensive (~15GB/thread)\n",
    "SALMON_THREADS = 10\n",
    "\n",
    "# Create output directories\n",
    "for name, path in DIRS.items():\n",
    "    if name not in ['data', 'genome']:  # Don't create input dirs\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "# Create subdirectories\n",
    "(DIRS['qc'] / 'fastqc_raw').mkdir(exist_ok=True)\n",
    "(DIRS['qc'] / 'fastqc_trimmed').mkdir(exist_ok=True)\n",
    "(DIRS['qc'] / 'multiqc_reports').mkdir(exist_ok=True)\n",
    "(DIRS['rrna_filtered'] / 'non_rRNA').mkdir(exist_ok=True)\n",
    "(DIRS['rrna_filtered'] / 'rRNA').mkdir(exist_ok=True)\n",
    "(DIRS['salmon'] / 'index').mkdir(exist_ok=True)\n",
    "(DIRS['salmon'] / 'quants').mkdir(exist_ok=True)\n",
    "(DIRS['alignment'] / 'bowtie2_index').mkdir(exist_ok=True)\n",
    "(DIRS['alignment'] / 'bam').mkdir(exist_ok=True)\n",
    "(DIRS['alignment'] / 'bigwig').mkdir(exist_ok=True)\n",
    "(DIRS['figures'] / 'qc_plots').mkdir(exist_ok=True)\n",
    "(DIRS['figures'] / 'volcano_plots').mkdir(exist_ok=True)\n",
    "(DIRS['figures'] / 'heatmaps').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Output directories created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sample Metadata ===\n",
    "\n",
    "# Define experimental design\n",
    "sample_info = {\n",
    "    # Group 1 - Nutrients (Control: U4,5,6)\n",
    "    'U4': {'group': 1, 'condition': 'Control_MAD', 'replicate': 1},\n",
    "    'U5': {'group': 1, 'condition': 'Control_MAD', 'replicate': 2},\n",
    "    'U6': {'group': 1, 'condition': 'Control_MAD', 'replicate': 3},\n",
    "    'U46': {'group': 1, 'condition': 'Glycerol_0.75pct', 'replicate': 1},\n",
    "    'U47': {'group': 1, 'condition': 'Glycerol_0.75pct', 'replicate': 2},\n",
    "    'U48': {'group': 1, 'condition': 'Glycerol_0.75pct', 'replicate': 3},\n",
    "    'U22': {'group': 1, 'condition': 'Low_Nitrogen', 'replicate': 1},\n",
    "    'U23': {'group': 1, 'condition': 'Low_Nitrogen', 'replicate': 2},\n",
    "    'U24': {'group': 1, 'condition': 'Low_Nitrogen', 'replicate': 3},\n",
    "    'U25': {'group': 1, 'condition': 'High_Nitrogen', 'replicate': 1},\n",
    "    'U26': {'group': 1, 'condition': 'High_Nitrogen', 'replicate': 2},\n",
    "    'U27': {'group': 1, 'condition': 'High_Nitrogen', 'replicate': 3},\n",
    "    'U28': {'group': 1, 'condition': 'Low_Phosphate', 'replicate': 1},\n",
    "    'U29': {'group': 1, 'condition': 'Low_Phosphate', 'replicate': 2},\n",
    "    'U30': {'group': 1, 'condition': 'Low_Phosphate', 'replicate': 3},\n",
    "    'U31': {'group': 1, 'condition': 'High_Phosphate', 'replicate': 1},\n",
    "    'U32': {'group': 1, 'condition': 'High_Phosphate', 'replicate': 2},\n",
    "    'U33': {'group': 1, 'condition': 'High_Phosphate', 'replicate': 3},\n",
    "    'U40': {'group': 1, 'condition': 'Ammonia', 'replicate': 1},\n",
    "    'U41': {'group': 1, 'condition': 'Ammonia', 'replicate': 2},\n",
    "    'U42': {'group': 1, 'condition': 'Ammonia', 'replicate': 3},\n",
    "    'U43': {'group': 1, 'condition': 'Urea', 'replicate': 1},\n",
    "    'U44': {'group': 1, 'condition': 'Urea', 'replicate': 2},\n",
    "    'U45': {'group': 1, 'condition': 'Urea', 'replicate': 3},\n",
    "    \n",
    "    # Group 2 - Environmental (Control: U1,2,3)\n",
    "    'U1': {'group': 2, 'condition': 'Control_MAD', 'replicate': 1},\n",
    "    'U2': {'group': 2, 'condition': 'Control_MAD', 'replicate': 2},\n",
    "    'U3': {'group': 2, 'condition': 'Control_MAD', 'replicate': 3},\n",
    "    'U34': {'group': 2, 'condition': 'High_NaCl_9pct', 'replicate': 1},\n",
    "    'U35': {'group': 2, 'condition': 'High_NaCl_9pct', 'replicate': 2},\n",
    "    'U36': {'group': 2, 'condition': 'High_NaCl_9pct', 'replicate': 3},\n",
    "    'U37': {'group': 2, 'condition': 'H2O2_0.005pct', 'replicate': 1},\n",
    "    'U38': {'group': 2, 'condition': 'H2O2_0.005pct', 'replicate': 2},\n",
    "    'U39': {'group': 2, 'condition': 'H2O2_0.005pct', 'replicate': 3},\n",
    "    'U7': {'group': 2, 'condition': 'Atmospheric_CO2', 'replicate': 1},\n",
    "    'U8': {'group': 2, 'condition': 'Atmospheric_CO2', 'replicate': 2},\n",
    "    'U9': {'group': 2, 'condition': 'Atmospheric_CO2', 'replicate': 3},\n",
    "    'U10': {'group': 2, 'condition': 'High_CO2_8pct', 'replicate': 1},\n",
    "    'U11': {'group': 2, 'condition': 'High_CO2_8pct', 'replicate': 2},\n",
    "    'U12': {'group': 2, 'condition': 'High_CO2_8pct', 'replicate': 3},\n",
    "    'U13': {'group': 2, 'condition': 'High_Temp_38C', 'replicate': 1},\n",
    "    'U14': {'group': 2, 'condition': 'High_Temp_38C', 'replicate': 2},\n",
    "    'U15': {'group': 2, 'condition': 'High_Temp_38C', 'replicate': 3},\n",
    "    'U16': {'group': 2, 'condition': 'Low_Light_15uE', 'replicate': 1},\n",
    "    'U17': {'group': 2, 'condition': 'Low_Light_15uE', 'replicate': 2},\n",
    "    'U18': {'group': 2, 'condition': 'Low_Light_15uE', 'replicate': 3},\n",
    "    'U19': {'group': 2, 'condition': 'High_Light', 'replicate': 1},\n",
    "    'U20': {'group': 2, 'condition': 'High_Light', 'replicate': 2},\n",
    "    'U21': {'group': 2, 'condition': 'High_Light', 'replicate': 3},\n",
    "    \n",
    "    # Group 3 - Circadian\n",
    "    'U49': {'group': 3, 'condition': 'T1_Light', 'replicate': 1},\n",
    "    'U50': {'group': 3, 'condition': 'T1_Light', 'replicate': 2},\n",
    "    'U51': {'group': 3, 'condition': 'T1_Light', 'replicate': 3},\n",
    "    'U52': {'group': 3, 'condition': 'T2_Dark', 'replicate': 1},\n",
    "    'U53': {'group': 3, 'condition': 'T2_Dark', 'replicate': 2},\n",
    "    'U54': {'group': 3, 'condition': 'T2_Dark', 'replicate': 3},\n",
    "    'U55': {'group': 3, 'condition': 'T3_Light', 'replicate': 1},\n",
    "    'U56': {'group': 3, 'condition': 'T3_Light', 'replicate': 2},\n",
    "    'U57': {'group': 3, 'condition': 'T3_Light', 'replicate': 3},\n",
    "    'U58': {'group': 3, 'condition': 'T4_Dark', 'replicate': 1},\n",
    "    'U59': {'group': 3, 'condition': 'T4_Dark', 'replicate': 2},\n",
    "    'U60': {'group': 3, 'condition': 'T4_Dark', 'replicate': 3},\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "metadata = pd.DataFrame.from_dict(sample_info, orient='index')\n",
    "metadata.index.name = 'sample_id'\n",
    "metadata = metadata.reset_index()\n",
    "\n",
    "# Sort by sample number\n",
    "metadata['sample_num'] = metadata['sample_id'].str.extract(r'U(\\d+)').astype(int)\n",
    "metadata = metadata.sort_values('sample_num').reset_index(drop=True)\n",
    "\n",
    "print(f\"Total samples: {len(metadata)}\")\n",
    "print(f\"\\nGroup summary:\")\n",
    "print(metadata.groupby('group')['condition'].nunique())\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper Functions ===\n",
    "\n",
    "def run_command(cmd, description=\"Running command\", log_file=None, check=True):\n",
    "    \"\"\"\n",
    "    Run a shell command with optional logging.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cmd : str\n",
    "        Shell command to execute\n",
    "    description : str\n",
    "        Description for progress display\n",
    "    log_file : Path, optional\n",
    "        File to write stdout/stderr\n",
    "    check : bool\n",
    "        Raise exception on non-zero exit code\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    subprocess.CompletedProcess\n",
    "    \"\"\"\n",
    "    print(f\"  {description}...\")\n",
    "    \n",
    "    if log_file:\n",
    "        with open(log_file, 'w') as f:\n",
    "            result = subprocess.run(\n",
    "                cmd, shell=True, stdout=f, stderr=subprocess.STDOUT,\n",
    "                check=check\n",
    "            )\n",
    "    else:\n",
    "        result = subprocess.run(\n",
    "            cmd, shell=True, capture_output=True, text=True, check=check\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_fastq_files(sample_id, directory, pattern='merged'):\n",
    "    \"\"\"\n",
    "    Get FASTQ file paths for a sample.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_id : str\n",
    "        Sample ID (e.g., 'U1')\n",
    "    directory : Path\n",
    "        Directory containing FASTQ files\n",
    "    pattern : str\n",
    "        'raw' for lane-split files, 'merged' for merged files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with 'R1' and 'R2' keys\n",
    "    \"\"\"\n",
    "    if pattern == 'merged':\n",
    "        r1 = directory / f\"{sample_id}_R1.fastq.gz\"\n",
    "        r2 = directory / f\"{sample_id}_R2.fastq.gz\"\n",
    "        return {'R1': r1, 'R2': r2}\n",
    "    else:\n",
    "        # Raw files (4 per sample)\n",
    "        files = list(directory.glob(f\"{sample_id}-*.fastq.gz\"))\n",
    "        r1 = sorted([f for f in files if '_R1_' in f.name])\n",
    "        r2 = sorted([f for f in files if '_R2_' in f.name])\n",
    "        return {'R1': r1, 'R2': r2}\n",
    "\n",
    "\n",
    "def check_tool(tool_name):\n",
    "    \"\"\"Check if a tool is available in PATH.\"\"\"\n",
    "    result = subprocess.run(f\"which {tool_name}\", shell=True, capture_output=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  [OK] {tool_name}: {result.stdout.decode().strip()}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"  [MISSING] {tool_name}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verify Tool Installation ===\n",
    "\n",
    "print(\"Checking required tools...\\n\")\n",
    "\n",
    "tools = [\n",
    "    'fastqc', 'multiqc', 'fastp',  # QC\n",
    "    'sortmerna',                     # rRNA removal\n",
    "    'gffread',                       # Transcriptome extraction\n",
    "    'salmon',                        # Quantification\n",
    "    'bowtie2', 'samtools',           # Alignment\n",
    "    'bamCoverage',                   # deepTools\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for tool in tools:\n",
    "    if not check_tool(tool):\n",
    "        missing.append(tool)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n[WARNING] Missing tools: {missing}\")\n",
    "    print(\"Install with: conda activate pcc11901_rnaseq\")\n",
    "else:\n",
    "    print(\"\\nAll tools available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Pre-processing\n",
    "\n",
    "### 1.1 Quality Control of Raw Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fastqc-raw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FastQC on Raw Reads ===\n",
    "# \n",
    "# Shell command equivalent:\n",
    "# fastqc -t 8 -o 01_QC/fastqc_raw/ Data/*.fastq.gz\n",
    "#\n",
    "# This step checks:\n",
    "# - Per-base sequence quality\n",
    "# - Per-sequence quality scores\n",
    "# - GC content distribution\n",
    "# - Sequence duplication levels\n",
    "# - Overrepresented sequences (adapters)\n",
    "# - Adapter content\n",
    "\n",
    "RUN_FASTQC_RAW = False  # Set to True to run\n",
    "\n",
    "if RUN_FASTQC_RAW:\n",
    "    output_dir = DIRS['qc'] / 'fastqc_raw'\n",
    "    input_files = list(DIRS['data'].glob('*.fastq.gz'))\n",
    "    \n",
    "    print(f\"Running FastQC on {len(input_files)} files...\")\n",
    "    print(f\"Output: {output_dir}\")\n",
    "    \n",
    "    cmd = f\"fastqc -t {N_THREADS} -o {output_dir} {DIRS['data']}/*.fastq.gz\"\n",
    "    print(f\"\\nCommand:\\n{cmd}\\n\")\n",
    "    \n",
    "    # Run FastQC\n",
    "    !{cmd}\n",
    "    \n",
    "    print(\"\\nFastQC complete!\")\n",
    "else:\n",
    "    print(\"Skipping FastQC on raw reads (set RUN_FASTQC_RAW = True to run)\")\n",
    "    print(f\"\\nTo run manually:\\nfastqc -t {N_THREADS} -o {DIRS['qc']}/fastqc_raw/ {DIRS['data']}/*.fastq.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trim-header",
   "metadata": {},
   "source": [
    "### 1.2 Trimming with fastp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fastp-trimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Trimming with fastp ===\n",
    "#\n",
    "# fastp automatically:\n",
    "# - Detects and removes adapters\n",
    "# - Trims low-quality bases from both ends\n",
    "# - Filters reads below quality threshold\n",
    "# - Generates HTML/JSON QC reports\n",
    "#\n",
    "# For each sample, we process the 4 raw FASTQ files (2 lanes × 2 reads)\n",
    "# and output trimmed files ready for lane merging.\n",
    "\n",
    "RUN_FASTP = False  # Set to True to run\n",
    "\n",
    "def run_fastp_sample(sample_id):\n",
    "    \"\"\"\n",
    "    Run fastp on a single sample (all lanes).\n",
    "    \"\"\"\n",
    "    raw_files = get_fastq_files(sample_id, DIRS['data'], pattern='raw')\n",
    "    \n",
    "    # Process each lane separately\n",
    "    for lane in ['L001', 'L002']:\n",
    "        r1_in = [f for f in raw_files['R1'] if lane in f.name][0]\n",
    "        r2_in = [f for f in raw_files['R2'] if lane in f.name][0]\n",
    "        \n",
    "        r1_out = DIRS['trimmed'] / f\"{sample_id}_{lane}_R1.fastq.gz\"\n",
    "        r2_out = DIRS['trimmed'] / f\"{sample_id}_{lane}_R2.fastq.gz\"\n",
    "        \n",
    "        json_report = DIRS['trimmed'] / f\"{sample_id}_{lane}.fastp.json\"\n",
    "        html_report = DIRS['trimmed'] / f\"{sample_id}_{lane}.fastp.html\"\n",
    "        \n",
    "        cmd = f\"\"\"\n",
    "fastp \\\\\n",
    "    -i \"{r1_in}\" \\\\\n",
    "    -I \"{r2_in}\" \\\\\n",
    "    -o \"{r1_out}\" \\\\\n",
    "    -O \"{r2_out}\" \\\\\n",
    "    --detect_adapter_for_pe \\\\\n",
    "    --correction \\\\\n",
    "    --qualified_quality_phred 20 \\\\\n",
    "    --length_required 50 \\\\\n",
    "    --thread {N_THREADS} \\\\\n",
    "    --json \"{json_report}\" \\\\\n",
    "    --html \"{html_report}\"\n",
    "\"\"\"\n",
    "        subprocess.run(cmd, shell=True, check=True, capture_output=True)\n",
    "    \n",
    "    return sample_id\n",
    "\n",
    "\n",
    "if RUN_FASTP:\n",
    "    samples = metadata['sample_id'].tolist()\n",
    "    print(f\"Running fastp on {len(samples)} samples...\")\n",
    "    print(f\"Output: {DIRS['trimmed']}\")\n",
    "    \n",
    "    # Run in parallel (but fastp itself uses threads, so limit parallel jobs)\n",
    "    results = Parallel(n_jobs=2)(\n",
    "        delayed(run_fastp_sample)(s) for s in tqdm(samples, desc=\"Trimming\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCompleted trimming for {len(results)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping fastp trimming (set RUN_FASTP = True to run)\")\n",
    "    print(\"\\nExample command for single sample:\")\n",
    "    print(f\"\"\"\n",
    "fastp \\\\\n",
    "    -i Data/U1-AMO17076A1-22FLL5LT1_S1_L001_R1_001.fastq.gz \\\\\n",
    "    -I Data/U1-AMO17076A1-22FLL5LT1_S1_L001_R2_001.fastq.gz \\\\\n",
    "    -o 02_trimmed/U1_L001_R1.fastq.gz \\\\\n",
    "    -O 02_trimmed/U1_L001_R2.fastq.gz \\\\\n",
    "    --detect_adapter_for_pe \\\\\n",
    "    --qualified_quality_phred 20 \\\\\n",
    "    --thread {N_THREADS}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-header",
   "metadata": {},
   "source": [
    "### 1.3 Lane Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lane-merging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Lane Merging ===\n",
    "#\n",
    "# Merge L001 and L002 files for each sample.\n",
    "# Result: 120 files (60 samples × 2 reads)\n",
    "#\n",
    "# Shell command:\n",
    "# cat U1_L001_R1.fastq.gz U1_L002_R1.fastq.gz > U1_R1.fastq.gz\n",
    "\n",
    "RUN_MERGE = False  # Set to True to run\n",
    "\n",
    "def merge_lanes(sample_id):\n",
    "    \"\"\"\n",
    "    Merge lane files for a single sample.\n",
    "    \"\"\"\n",
    "    for read in ['R1', 'R2']:\n",
    "        l001 = DIRS['trimmed'] / f\"{sample_id}_L001_{read}.fastq.gz\"\n",
    "        l002 = DIRS['trimmed'] / f\"{sample_id}_L002_{read}.fastq.gz\"\n",
    "        merged = DIRS['merged'] / f\"{sample_id}_{read}.fastq.gz\"\n",
    "        \n",
    "        # Use cat (gzipped files can be concatenated directly)\n",
    "        cmd = f'cat \"{l001}\" \"{l002}\" > \"{merged}\"'\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "    \n",
    "    return sample_id\n",
    "\n",
    "\n",
    "if RUN_MERGE:\n",
    "    samples = metadata['sample_id'].tolist()\n",
    "    print(f\"Merging lanes for {len(samples)} samples...\")\n",
    "    \n",
    "    results = Parallel(n_jobs=N_THREADS)(\n",
    "        delayed(merge_lanes)(s) for s in tqdm(samples, desc=\"Merging\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMerged files saved to: {DIRS['merged']}\")\n",
    "else:\n",
    "    print(\"Skipping lane merging (set RUN_MERGE = True to run)\")\n",
    "    print(\"\\nExample commands:\")\n",
    "    print(\"cat 02_trimmed/U1_L001_R1.fastq.gz 02_trimmed/U1_L002_R1.fastq.gz > 03_merged/U1_R1.fastq.gz\")\n",
    "    print(\"cat 02_trimmed/U1_L001_R2.fastq.gz 02_trimmed/U1_L002_R2.fastq.gz > 03_merged/U1_R2.fastq.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rrna-header",
   "metadata": {},
   "source": [
    "### 1.4 rRNA Removal with SortMeRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sortmerna",
   "metadata": {},
   "outputs": [],
   "source": "# === rRNA Removal with SortMeRNA ===\n#\n# SortMeRNA filters out ribosomal RNA reads.\n# This is critical for bacterial RNA-seq even after rRNA depletion during library prep.\n#\n# IMPORTANT: Memory-intensive! Use max 3-4 threads (~15GB RAM each)\n# IMPORTANT: This step takes ~45-60 min PER SAMPLE. Running 60 samples = ~2.5 days!\n#            DO NOT run in the notebook kernel - use the generated shell script instead.\n#\n# Shell command:\n# sortmerna \\\n#     --ref silva-bac-16s-id90.fasta \\\n#     --ref silva-bac-23s-id98.fasta \\\n#     --reads U1_R1.fastq.gz --reads U1_R2.fastq.gz \\\n#     --paired_in --out2 \\\n#     --aligned rRNA/U1 \\\n#     --other non_rRNA/U1 \\\n#     --fastx --threads 4\n\nGENERATE_SORTMERNA_SCRIPT = True  # Generate shell script for external execution\n\n# SortMeRNA database paths (adjust based on your conda environment)\nSORTMERNA_DB = os.environ.get('CONDA_PREFIX', '') + '/share/sortmerna/rRNA_databases'\n\nif GENERATE_SORTMERNA_SCRIPT:\n    samples = metadata['sample_id'].tolist()\n    script_path = DIRS['logs'] / \"run_sortmerna_all.sh\"\n    \n    with open(script_path, \"w\") as f:\n        f.write(\"#!/bin/bash\\n\")\n        f.write(\"# ==============================================\\n\")\n        f.write(\"# SortMeRNA Batch Script - PCC 11901 RNA-seq\\n\")\n        f.write(\"# ==============================================\\n\")\n        f.write(\"# IMPORTANT: Run this in a separate terminal, NOT in Jupyter!\\n\")\n        f.write(\"# Estimated runtime: ~45-60 hours (60 samples × ~1 hour each)\\n\")\n        f.write(\"#\\n\")\n        f.write(\"# Usage:\\n\")\n        f.write(\"#   conda activate pcc11901_rnaseq\\n\")\n        f.write(f\"#   bash {script_path}\\n\")\n        f.write(\"#\\n\")\n        f.write(\"# To resume after interruption, comment out completed samples.\\n\")\n        f.write(\"# ==============================================\\n\\n\")\n        f.write(\"set -e  # Exit on error\\n\\n\")\n        f.write(f\"# SortMeRNA database path\\n\")\n        f.write(f\"SORTMERNA_DB=\\\"$CONDA_PREFIX/share/sortmerna/rRNA_databases\\\"\\n\\n\")\n        f.write(f\"# Verify database exists\\n\")\n        f.write(f\"if [ ! -d \\\"$SORTMERNA_DB\\\" ]; then\\n\")\n        f.write(f\"    echo \\\"ERROR: SortMeRNA database not found at $SORTMERNA_DB\\\"\\n\")\n        f.write(f\"    exit 1\\n\")\n        f.write(f\"fi\\n\\n\")\n        f.write(f\"echo \\\"Starting SortMeRNA processing for {len(samples)} samples...\\\"\\n\")\n        f.write(f\"echo \\\"Start time: $(date)\\\"\\n\\n\")\n        \n        for i, sample in enumerate(samples, 1):\n            r1 = DIRS['merged'] / f\"{sample}_R1.fastq.gz\"\n            r2 = DIRS['merged'] / f\"{sample}_R2.fastq.gz\"\n            aligned_prefix = DIRS['rrna_filtered'] / 'rRNA' / sample\n            other_prefix = DIRS['rrna_filtered'] / 'non_rRNA' / sample\n            workdir = DIRS['rrna_filtered'] / f'workdir_{sample}'\n            log_file = DIRS['logs'] / f'{sample}_sortmerna.log'\n            \n            f.write(f\"# === Sample {i}/{len(samples)}: {sample} ===\\n\")\n            f.write(f\"echo \\\"[{i}/{len(samples)}] Processing {sample}...\\\"\\n\")\n            f.write(f\"mkdir -p \\\"{workdir}\\\"\\n\")\n            f.write(f\"sortmerna \\\\\\n\")\n            f.write(f\"    --ref \\\"$SORTMERNA_DB/silva-bac-16s-id90.fasta\\\" \\\\\\n\")\n            f.write(f\"    --ref \\\"$SORTMERNA_DB/silva-bac-23s-id98.fasta\\\" \\\\\\n\")\n            f.write(f\"    --reads \\\"{r1}\\\" --reads \\\"{r2}\\\" \\\\\\n\")\n            f.write(f\"    --paired_in --out2 \\\\\\n\")\n            f.write(f\"    --aligned \\\"{aligned_prefix}\\\" \\\\\\n\")\n            f.write(f\"    --other \\\"{other_prefix}\\\" \\\\\\n\")\n            f.write(f\"    --fastx \\\\\\n\")\n            f.write(f\"    --threads {SORTMERNA_THREADS} \\\\\\n\")\n            f.write(f\"    --workdir \\\"{workdir}\\\" \\\\\\n\")\n            f.write(f\"    2>&1 | tee \\\"{log_file}\\\"\\n\")\n            f.write(f\"rm -rf \\\"{workdir}\\\"\\n\")\n            f.write(f\"echo \\\"  Completed {sample} at $(date)\\\"\\n\\n\")\n        \n        f.write(\"echo \\\"==============================================\\\"\\n\")\n        f.write(\"echo \\\"SortMeRNA processing complete!\\\"\\n\")\n        f.write(\"echo \\\"End time: $(date)\\\"\\n\")\n        f.write(\"echo \\\"==============================================\\\"\\n\")\n    \n    # Make executable\n    os.chmod(script_path, 0o755)\n    \n    print(f\"Generated SortMeRNA batch script: {script_path}\")\n    print(f\"\\nTo run (in a separate terminal):\")\n    print(f\"  conda activate pcc11901_rnaseq\")\n    print(f\"  bash {script_path}\")\n    print(f\"\\nEstimated runtime: ~45-60 hours for {len(samples)} samples\")\n    print(\"TIP: Use 'screen' or 'tmux' to keep it running if you disconnect.\")\nelse:\n    print(\"Skipping SortMeRNA script generation (set GENERATE_SORTMERNA_SCRIPT = True)\")\n    print(\"\\nExample command for single sample:\")\n    print(f\"\"\"\nsortmerna \\\\\n    --ref $CONDA_PREFIX/share/sortmerna/rRNA_databases/silva-bac-16s-id90.fasta \\\\\n    --ref $CONDA_PREFIX/share/sortmerna/rRNA_databases/silva-bac-23s-id98.fasta \\\\\n    --reads 03_merged/U1_R1.fastq.gz --reads 03_merged/U1_R2.fastq.gz \\\\\n    --paired_in --out2 \\\\\n    --aligned 04_rRNA_filtered/rRNA/U1 \\\\\n    --other 04_rRNA_filtered/non_rRNA/U1 \\\\\n    --fastx --threads {SORTMERNA_THREADS}\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "fastqc-post-header",
   "metadata": {},
   "source": [
    "### 1.5 Post-Processing QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fastqc-trimmed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FastQC on Trimmed/Filtered Reads ===\n",
    "#\n",
    "# Run FastQC on the rRNA-filtered reads to verify quality improvement.\n",
    "\n",
    "RUN_FASTQC_TRIMMED = False  # Set to True to run\n",
    "\n",
    "if RUN_FASTQC_TRIMMED:\n",
    "    output_dir = DIRS['qc'] / 'fastqc_trimmed'\n",
    "    input_dir = DIRS['rrna_filtered'] / 'non_rRNA'\n",
    "    \n",
    "    cmd = f\"fastqc -t {N_THREADS} -o {output_dir} {input_dir}/*.fastq.gz\"\n",
    "    print(f\"Running FastQC on filtered reads...\")\n",
    "    print(f\"Command: {cmd}\\n\")\n",
    "    \n",
    "    !{cmd}\n",
    "    \n",
    "    print(\"\\nFastQC complete!\")\n",
    "else:\n",
    "    print(\"Skipping post-filter FastQC (set RUN_FASTQC_TRIMMED = True to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiqc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MultiQC Summary Report ===\n",
    "#\n",
    "# Aggregate all QC reports into a single interactive report.\n",
    "\n",
    "RUN_MULTIQC = False  # Set to True to run\n",
    "\n",
    "if RUN_MULTIQC:\n",
    "    output_dir = DIRS['qc'] / 'multiqc_reports'\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "multiqc \\\\\n",
    "    {DIRS['qc']}/fastqc_raw \\\\\n",
    "    {DIRS['qc']}/fastqc_trimmed \\\\\n",
    "    {DIRS['trimmed']} \\\\\n",
    "    -o {output_dir} \\\\\n",
    "    --filename multiqc_report \\\\\n",
    "    --force\n",
    "\"\"\"\n",
    "    print(\"Running MultiQC...\")\n",
    "    !{cmd}\n",
    "    \n",
    "    print(f\"\\nReport saved to: {output_dir}/multiqc_report.html\")\n",
    "else:\n",
    "    print(\"Skipping MultiQC (set RUN_MULTIQC = True to run)\")\n",
    "    print(f\"\\nTo run manually:\\nmultiqc {DIRS['qc']}/ -o {DIRS['qc']}/multiqc_reports/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: Quantification and Alignment\n",
    "\n",
    "### 2.1 Extract Transcriptome (gffread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-transcriptome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract Transcriptome with gffread ===\n",
    "#\n",
    "# Create a FASTA file of transcript sequences from the genome + GTF.\n",
    "# This is required for Salmon indexing.\n",
    "#\n",
    "# Shell command:\n",
    "# gffread -w transcriptome.fa -g genome.fna annotation.gtf\n",
    "\n",
    "RUN_GFFREAD = False  # Set to True to run\n",
    "\n",
    "if RUN_GFFREAD:\n",
    "    cmd = f\"\"\"\n",
    "gffread \\\\\n",
    "    -w \"{TRANSCRIPTOME_FASTA}\" \\\\\n",
    "    -g \"{GENOME_FASTA}\" \\\\\n",
    "    \"{GTF_FILE}\"\n",
    "\"\"\"\n",
    "    print(\"Extracting transcriptome...\")\n",
    "    print(f\"Command: {cmd}\")\n",
    "    \n",
    "    !{cmd}\n",
    "    \n",
    "    # Check result\n",
    "    if TRANSCRIPTOME_FASTA.exists():\n",
    "        n_transcripts = !grep -c \"^>\" \"{TRANSCRIPTOME_FASTA}\"\n",
    "        print(f\"\\nTranscriptome created: {TRANSCRIPTOME_FASTA}\")\n",
    "        print(f\"Number of transcripts: {n_transcripts[0]}\")\n",
    "    else:\n",
    "        print(\"[ERROR] Transcriptome extraction failed!\")\n",
    "else:\n",
    "    print(\"Skipping transcriptome extraction (set RUN_GFFREAD = True to run)\")\n",
    "    print(f\"\\nTo run manually:\")\n",
    "    print(f'gffread -w \"{TRANSCRIPTOME_FASTA}\" -g \"{GENOME_FASTA}\" \"{GTF_FILE}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "salmon-header",
   "metadata": {},
   "source": [
    "### 2.2 Salmon Index and Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "salmon-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build Salmon Index ===\n",
    "#\n",
    "# Using decoy-aware mapping for better accuracy:\n",
    "# - Target: transcriptome sequences\n",
    "# - Decoy: whole genome (prevents false quantification of intergenic reads)\n",
    "#\n",
    "# Shell commands:\n",
    "# grep \"^>\" genome.fna | cut -d \" \" -f 1 | sed 's/>//g' > decoys.txt\n",
    "# cat transcriptome.fa genome.fna > gentrome.fa\n",
    "# salmon index -t gentrome.fa -d decoys.txt -i salmon_index -p 8\n",
    "\n",
    "RUN_SALMON_INDEX = False  # Set to True to run\n",
    "\n",
    "SALMON_INDEX = DIRS['salmon'] / 'index'\n",
    "GENTROME = DIRS['genome'] / 'gentrome.fa'\n",
    "DECOYS = DIRS['genome'] / 'decoys.txt'\n",
    "\n",
    "if RUN_SALMON_INDEX:\n",
    "    print(\"Building Salmon index with decoy-aware mapping...\\n\")\n",
    "    \n",
    "    # Step 1: Create decoys.txt (chromosome names)\n",
    "    print(\"Step 1: Creating decoys list...\")\n",
    "    cmd_decoys = f'grep \"^>\" \"{GENOME_FASTA}\" | cut -d \" \" -f 1 | sed \"s/>//g\" > \"{DECOYS}\"'\n",
    "    !{cmd_decoys}\n",
    "    print(f\"  Decoys file: {DECOYS}\")\n",
    "    \n",
    "    # Step 2: Create gentrome (transcriptome + genome)\n",
    "    print(\"\\nStep 2: Creating gentrome...\")\n",
    "    cmd_gentrome = f'cat \"{TRANSCRIPTOME_FASTA}\" \"{GENOME_FASTA}\" > \"{GENTROME}\"'\n",
    "    !{cmd_gentrome}\n",
    "    print(f\"  Gentrome file: {GENTROME}\")\n",
    "    \n",
    "    # Step 3: Build Salmon index\n",
    "    print(f\"\\nStep 3: Building Salmon index (this may take a few minutes)...\")\n",
    "    cmd_index = f\"\"\"\n",
    "salmon index \\\\\n",
    "    -t \"{GENTROME}\" \\\\\n",
    "    -d \"{DECOYS}\" \\\\\n",
    "    -i \"{SALMON_INDEX}\" \\\\\n",
    "    -p {SALMON_THREADS}\n",
    "\"\"\"\n",
    "    print(f\"Command: {cmd_index}\")\n",
    "    !{cmd_index}\n",
    "    \n",
    "    print(f\"\\nSalmon index created: {SALMON_INDEX}\")\n",
    "else:\n",
    "    print(\"Skipping Salmon index build (set RUN_SALMON_INDEX = True to run)\")\n",
    "    print(\"\\nCommands to run manually:\")\n",
    "    print(f'grep \"^>\" \"{GENOME_FASTA}\" | cut -d \" \" -f 1 | sed \"s/>//g\" > \"{DECOYS}\"')\n",
    "    print(f'cat \"{TRANSCRIPTOME_FASTA}\" \"{GENOME_FASTA}\" > \"{GENTROME}\"')\n",
    "    print(f'salmon index -t \"{GENTROME}\" -d \"{DECOYS}\" -i \"{SALMON_INDEX}\" -p {SALMON_THREADS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "salmon-quant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Salmon Quantification ===\n",
    "#\n",
    "# Quantify transcript abundance for each sample.\n",
    "# Salmon auto-detects library type (strandedness).\n",
    "#\n",
    "# Shell command:\n",
    "# salmon quant -i salmon_index -l A \\\n",
    "#     -1 U1_R1.fastq.gz -2 U1_R2.fastq.gz \\\n",
    "#     -o quants/U1 --validateMappings --gcBias -p 10\n",
    "\n",
    "RUN_SALMON_QUANT = False  # Set to True to run\n",
    "\n",
    "def run_salmon_quant(sample_id):\n",
    "    \"\"\"\n",
    "    Run Salmon quantification on a single sample.\n",
    "    Uses rRNA-filtered reads from 04_rRNA_filtered/non_rRNA/\n",
    "    \"\"\"\n",
    "    # Input files (from SortMeRNA output)\n",
    "    r1 = DIRS['rrna_filtered'] / 'non_rRNA' / f\"{sample_id}_fwd.fq.gz\"\n",
    "    r2 = DIRS['rrna_filtered'] / 'non_rRNA' / f\"{sample_id}_rev.fq.gz\"\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = DIRS['salmon'] / 'quants' / sample_id\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "salmon quant \\\\\n",
    "    -i \"{SALMON_INDEX}\" \\\\\n",
    "    -l A \\\\\n",
    "    -1 \"{r1}\" \\\\\n",
    "    -2 \"{r2}\" \\\\\n",
    "    -o \"{output_dir}\" \\\\\n",
    "    --validateMappings \\\\\n",
    "    --gcBias \\\\\n",
    "    --seqBias \\\\\n",
    "    -p {SALMON_THREADS}\n",
    "\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return sample_id\n",
    "\n",
    "\n",
    "if RUN_SALMON_QUANT:\n",
    "    samples = metadata['sample_id'].tolist()\n",
    "    print(f\"Running Salmon quantification on {len(samples)} samples...\")\n",
    "    print(f\"Output: {DIRS['salmon']}/quants/\")\n",
    "    \n",
    "    # Run sequentially (Salmon already uses multiple threads)\n",
    "    for sample in tqdm(samples, desc=\"Salmon quant\"):\n",
    "        run_salmon_quant(sample)\n",
    "    \n",
    "    print(\"\\nSalmon quantification complete!\")\n",
    "else:\n",
    "    print(\"Skipping Salmon quantification (set RUN_SALMON_QUANT = True to run)\")\n",
    "    print(\"\\nExample command for single sample:\")\n",
    "    print(f\"\"\"\n",
    "salmon quant \\\\\n",
    "    -i \"{SALMON_INDEX}\" \\\\\n",
    "    -l A \\\\\n",
    "    -1 04_rRNA_filtered/non_rRNA/U1_fwd.fq.gz \\\\\n",
    "    -2 04_rRNA_filtered/non_rRNA/U1_rev.fq.gz \\\\\n",
    "    -o 05_salmon/quants/U1 \\\\\n",
    "    --validateMappings --gcBias -p {SALMON_THREADS}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bowtie2-header",
   "metadata": {},
   "source": [
    "### 2.3 Bowtie2 Alignment (for Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bowtie2-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build Bowtie2 Index ===\n",
    "#\n",
    "# Build genome index for read alignment.\n",
    "# This is needed to create BAM files for IGV visualization.\n",
    "#\n",
    "# Shell command:\n",
    "# bowtie2-build genome.fna bowtie2_index/pcc11901\n",
    "\n",
    "RUN_BOWTIE2_INDEX = False  # Set to True to run\n",
    "\n",
    "BOWTIE2_INDEX = DIRS['alignment'] / 'bowtie2_index' / 'pcc11901'\n",
    "\n",
    "if RUN_BOWTIE2_INDEX:\n",
    "    print(\"Building Bowtie2 index...\")\n",
    "    \n",
    "    cmd = f'bowtie2-build --threads {N_THREADS} \"{GENOME_FASTA}\" \"{BOWTIE2_INDEX}\"'\n",
    "    print(f\"Command: {cmd}\\n\")\n",
    "    \n",
    "    !{cmd}\n",
    "    \n",
    "    print(f\"\\nBowtie2 index created: {BOWTIE2_INDEX}\")\n",
    "else:\n",
    "    print(\"Skipping Bowtie2 index (set RUN_BOWTIE2_INDEX = True to run)\")\n",
    "    print(f'\\nTo run manually:\\nbowtie2-build --threads {N_THREADS} \"{GENOME_FASTA}\" \"{BOWTIE2_INDEX}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bowtie2-align",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Bowtie2 Alignment ===\n",
    "#\n",
    "# Align reads to genome and create sorted BAM files.\n",
    "#\n",
    "# Shell command:\n",
    "# bowtie2 -x bowtie2_index/pcc11901 -1 R1.fq.gz -2 R2.fq.gz -p 8 | \\\n",
    "#     samtools view -bS - | samtools sort -o sample.bam\n",
    "# samtools index sample.bam\n",
    "\n",
    "RUN_BOWTIE2_ALIGN = False  # Set to True to run\n",
    "\n",
    "def run_bowtie2_align(sample_id):\n",
    "    \"\"\"\n",
    "    Align reads with Bowtie2 and create sorted BAM.\n",
    "    \"\"\"\n",
    "    r1 = DIRS['rrna_filtered'] / 'non_rRNA' / f\"{sample_id}_fwd.fq.gz\"\n",
    "    r2 = DIRS['rrna_filtered'] / 'non_rRNA' / f\"{sample_id}_rev.fq.gz\"\n",
    "    \n",
    "    bam_file = DIRS['alignment'] / 'bam' / f\"{sample_id}.bam\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "bowtie2 \\\\\n",
    "    -x \"{BOWTIE2_INDEX}\" \\\\\n",
    "    -1 \"{r1}\" \\\\\n",
    "    -2 \"{r2}\" \\\\\n",
    "    -p {N_THREADS} \\\\\n",
    "    2> \"{DIRS['logs']}/{sample_id}_bowtie2.log\" | \\\n",
    "samtools view -bS - | \\\n",
    "samtools sort -@ 4 -o \"{bam_file}\" -\n",
    "\n",
    "samtools index \"{bam_file}\"\n",
    "\"\"\"\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "    return sample_id\n",
    "\n",
    "\n",
    "if RUN_BOWTIE2_ALIGN:\n",
    "    samples = metadata['sample_id'].tolist()\n",
    "    print(f\"Running Bowtie2 alignment on {len(samples)} samples...\")\n",
    "    \n",
    "    for sample in tqdm(samples, desc=\"Aligning\"):\n",
    "        run_bowtie2_align(sample)\n",
    "    \n",
    "    print(f\"\\nBAM files saved to: {DIRS['alignment']}/bam/\")\n",
    "else:\n",
    "    print(\"Skipping Bowtie2 alignment (set RUN_BOWTIE2_ALIGN = True to run)\")\n",
    "    print(\"\\nExample command:\")\n",
    "    print(f\"\"\"\n",
    "bowtie2 -x \"{BOWTIE2_INDEX}\" \\\\\n",
    "    -1 04_rRNA_filtered/non_rRNA/U1_fwd.fq.gz \\\\\n",
    "    -2 04_rRNA_filtered/non_rRNA/U1_rev.fq.gz \\\\\n",
    "    -p {N_THREADS} | samtools view -bS - | samtools sort -o 06_alignment/bam/U1.bam -\n",
    "samtools index 06_alignment/bam/U1.bam\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigwig",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create bigWig Coverage Tracks ===\n",
    "#\n",
    "# Convert BAM to normalized bigWig for IGV visualization.\n",
    "# Using CPM (counts per million) normalization.\n",
    "#\n",
    "# Shell command:\n",
    "# bamCoverage -b sample.bam -o sample.bw --normalizeUsing CPM -p 8\n",
    "\n",
    "RUN_BIGWIG = False  # Set to True to run\n",
    "\n",
    "def create_bigwig(sample_id):\n",
    "    \"\"\"\n",
    "    Create CPM-normalized bigWig from BAM.\n",
    "    \"\"\"\n",
    "    bam_file = DIRS['alignment'] / 'bam' / f\"{sample_id}.bam\"\n",
    "    bigwig_file = DIRS['alignment'] / 'bigwig' / f\"{sample_id}.bw\"\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "bamCoverage \\\\\n",
    "    -b \"{bam_file}\" \\\\\n",
    "    -o \"{bigwig_file}\" \\\\\n",
    "    --normalizeUsing CPM \\\\\n",
    "    -p {N_THREADS}\n",
    "\"\"\"\n",
    "    subprocess.run(cmd, shell=True, check=True, capture_output=True)\n",
    "    return sample_id\n",
    "\n",
    "\n",
    "if RUN_BIGWIG:\n",
    "    samples = metadata['sample_id'].tolist()\n",
    "    print(f\"Creating bigWig files for {len(samples)} samples...\")\n",
    "    \n",
    "    for sample in tqdm(samples, desc=\"bigWig\"):\n",
    "        create_bigwig(sample)\n",
    "    \n",
    "    print(f\"\\nbigWig files saved to: {DIRS['alignment']}/bigwig/\")\n",
    "else:\n",
    "    print(\"Skipping bigWig creation (set RUN_BIGWIG = True to run)\")\n",
    "    print(f\"\\nTo run manually:\\nbamCoverage -b 06_alignment/bam/U1.bam -o 06_alignment/bigwig/U1.bw --normalizeUsing CPM -p {N_THREADS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Differential Expression Analysis\n",
    "\n",
    "### 3.1 Import Salmon Counts with tximport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup R Environment via rpy2 ===\n",
    "\n",
    "# Import R packages\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "\n",
    "# Load Bioconductor packages\n",
    "try:\n",
    "    tximport = importr('tximport')\n",
    "    deseq2 = importr('DESeq2')\n",
    "    print(\"[OK] tximport and DESeq2 loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not load R packages: {e}\")\n",
    "    print(\"Install with: R -e 'BiocManager::install(c(\\\"tximport\\\", \\\"DESeq2\\\"))'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-salmon-counts",
   "metadata": {},
   "outputs": [],
   "source": "# === Load Salmon Counts ===\n#\n# Use tximport to aggregate transcript-level counts to gene level.\n# This is the recommended way to import Salmon output into DESeq2.\n\nLOAD_COUNTS = False  # Set to True after Salmon quant is complete\n\nif LOAD_COUNTS:\n    # Create tx2gene mapping from GTF\n    print(\"Creating transcript-to-gene mapping...\")\n    \n    # Parse GTF to get transcript -> gene mapping\n    # Note: NCBI RefSeq GTF files may use different attribute formats\n    tx2gene = []\n    with open(GTF_FILE, 'r') as f:\n        for line in f:\n            if line.startswith('#'):\n                continue\n            fields = line.strip().split('\\t')\n            if len(fields) < 9:\n                continue\n            \n            # Only process CDS or exon features (which have transcript associations)\n            feature_type = fields[2]\n            if feature_type not in ['CDS', 'exon', 'transcript']:\n                continue\n                \n            attrs = fields[8]\n            \n            # Try multiple patterns for transcript_id (RefSeq GTFs vary)\n            tx_match = re.search(r'transcript_id \"([^\"]+)\"', attrs)\n            if not tx_match:\n                tx_match = re.search(r'transcript_id=([^;]+)', attrs)\n            \n            # Try multiple patterns for gene_id\n            gene_match = re.search(r'gene_id \"([^\"]+)\"', attrs)\n            if not gene_match:\n                gene_match = re.search(r'gene_id=([^;]+)', attrs)\n            if not gene_match:\n                # Fallback: try locus_tag\n                gene_match = re.search(r'locus_tag \"([^\"]+)\"', attrs)\n            \n            if tx_match and gene_match:\n                tx2gene.append({\n                    'transcript_id': tx_match.group(1),\n                    'gene_id': gene_match.group(1)\n                })\n    \n    tx2gene_df = pd.DataFrame(tx2gene).drop_duplicates()\n    \n    # === VALIDATION: Check tx2gene parsing succeeded ===\n    if len(tx2gene_df) == 0:\n        print(\"\\n[ERROR] tx2gene parsing failed! No transcript-gene mappings found.\")\n        print(\"Debugging info - first 5 lines of GTF attributes:\")\n        with open(GTF_FILE, 'r') as f:\n            for i, line in enumerate(f):\n                if not line.startswith('#') and i < 10:\n                    fields = line.strip().split('\\t')\n                    if len(fields) >= 9:\n                        print(f\"  {fields[8][:200]}...\")\n        raise ValueError(\n            \"tx2gene parsing failed! Check if your GTF uses 'gene_id'/'transcript_id' \"\n            \"or alternative formats like 'gene', 'locus_tag', etc. \"\n            \"Preview the GTF attributes above to debug.\"\n        )\n    \n    tx2gene_file = DIRS['deseq2'] / 'tx2gene.csv'\n    tx2gene_df.to_csv(tx2gene_file, index=False)\n    print(f\"  tx2gene mapping: {len(tx2gene_df)} transcripts -> {tx2gene_df['gene_id'].nunique()} genes\")\n    \n    # Get Salmon quant files\n    samples = metadata['sample_id'].tolist()\n    quant_files = [str(DIRS['salmon'] / 'quants' / s / 'quant.sf') for s in samples]\n    \n    # Check that files exist\n    missing = [f for f in quant_files if not Path(f).exists()]\n    if missing:\n        print(f\"[WARNING] Missing quant files: {len(missing)}\")\n        print(f\"  First missing: {missing[0]}\")\n    else:\n        print(f\"  Found all {len(quant_files)} quant.sf files\")\n    \n    # Import with tximport via R\n    print(\"\\nRunning tximport...\")\n    \n    ro.r(f'''\n    library(tximport)\n    \n    # Read tx2gene\n    tx2gene <- read.csv(\"{tx2gene_file}\")\n    \n    # Sample files\n    files <- c({', '.join([f'\"{f}\"' for f in quant_files])})\n    names(files) <- c({', '.join([f'\"{s}\"' for s in samples])})\n    \n    # Import\n    txi <- tximport(files, type=\"salmon\", tx2gene=tx2gene)\n    \n    # Save counts\n    counts <- as.data.frame(txi$counts)\n    write.csv(counts, \"{DIRS['deseq2']}/counts_matrix.csv\")\n    ''')\n    \n    # Load counts into Python\n    counts_df = pd.read_csv(DIRS['deseq2'] / 'counts_matrix.csv', index_col=0)\n    print(f\"\\nCounts matrix: {counts_df.shape[0]} genes × {counts_df.shape[1]} samples\")\n    counts_df.head()\nelse:\n    print(\"Skipping count loading (set LOAD_COUNTS = True after Salmon quant is complete)\")"
  },
  {
   "cell_type": "markdown",
   "id": "deseq2-header",
   "metadata": {},
   "source": [
    "### 3.2 DESeq2 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deseq2-group1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DESeq2 Analysis - Group 1 (Nutrients) ===\n",
    "#\n",
    "# Compare each nutrient condition vs Control (U4, U5, U6)\n",
    "#\n",
    "# Conditions:\n",
    "# - Glycerol_0.75pct\n",
    "# - Low_Nitrogen, High_Nitrogen\n",
    "# - Low_Phosphate, High_Phosphate\n",
    "# - Ammonia, Urea\n",
    "\n",
    "RUN_DESEQ2_GROUP1 = False  # Set to True to run\n",
    "\n",
    "if RUN_DESEQ2_GROUP1:\n",
    "    print(\"Running DESeq2 for Group 1 (Nutrients)...\\n\")\n",
    "    \n",
    "    # Filter metadata for Group 1\n",
    "    group1_meta = metadata[metadata['group'] == 1].copy()\n",
    "    group1_samples = group1_meta['sample_id'].tolist()\n",
    "    \n",
    "    # Save metadata for R\n",
    "    group1_meta_file = DIRS['deseq2'] / 'group1_metadata.csv'\n",
    "    group1_meta.to_csv(group1_meta_file, index=False)\n",
    "    \n",
    "    # Run DESeq2 in R\n",
    "    ro.r(f'''\n",
    "    library(DESeq2)\n",
    "    library(tximport)\n",
    "    \n",
    "    # Load metadata\n",
    "    coldata <- read.csv(\"{group1_meta_file}\")\n",
    "    rownames(coldata) <- coldata$sample_id\n",
    "    coldata$condition <- factor(coldata$condition)\n",
    "    coldata$condition <- relevel(coldata$condition, ref=\"Control_MAD\")\n",
    "    \n",
    "    # Load counts (subset to Group 1 samples)\n",
    "    counts <- read.csv(\"{DIRS['deseq2']}/counts_matrix.csv\", row.names=1)\n",
    "    counts <- counts[, coldata$sample_id]\n",
    "    counts <- round(counts)  # DESeq2 requires integers\n",
    "    \n",
    "    # Create DESeq2 object\n",
    "    dds <- DESeqDataSetFromMatrix(\n",
    "        countData = counts,\n",
    "        colData = coldata,\n",
    "        design = ~ condition\n",
    "    )\n",
    "    \n",
    "    # Filter low counts\n",
    "    keep <- rowSums(counts(dds) >= 10) >= 3\n",
    "    dds <- dds[keep,]\n",
    "    \n",
    "    # Run DESeq2\n",
    "    dds <- DESeq(dds)\n",
    "    \n",
    "    # Save normalized counts\n",
    "    norm_counts <- as.data.frame(counts(dds, normalized=TRUE))\n",
    "    write.csv(norm_counts, \"{DIRS['deseq2']}/group1_normalized_counts.csv\")\n",
    "    \n",
    "    # Extract results for each comparison\n",
    "    conditions <- levels(coldata$condition)\n",
    "    conditions <- conditions[conditions != \"Control_MAD\"]\n",
    "    \n",
    "    for (cond in conditions) {{\n",
    "        res <- results(dds, contrast=c(\"condition\", cond, \"Control_MAD\"))\n",
    "        res <- as.data.frame(res)\n",
    "        res$gene_id <- rownames(res)\n",
    "        write.csv(res, paste0(\"{DIRS['deseq2']}/group1_\", cond, \"_vs_Control.csv\"), row.names=FALSE)\n",
    "        \n",
    "        # Summary\n",
    "        sig <- sum(res$padj < 0.05 & abs(res$log2FoldChange) > 1, na.rm=TRUE)\n",
    "        cat(paste0(cond, \" vs Control: \", sig, \" DE genes\\n\"))\n",
    "    }}\n",
    "    ''')\n",
    "    \n",
    "    print(\"\\nGroup 1 analysis complete!\")\n",
    "    print(f\"Results saved to: {DIRS['deseq2']}/\")\n",
    "else:\n",
    "    print(\"Skipping DESeq2 Group 1 (set RUN_DESEQ2_GROUP1 = True to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deseq2-group2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DESeq2 Analysis - Group 2 (Environmental) ===\n",
    "#\n",
    "# Compare each environmental condition vs Control (U1, U2, U3)\n",
    "#\n",
    "# Conditions:\n",
    "# - High_NaCl_9pct, H2O2_0.005pct\n",
    "# - Atmospheric_CO2, High_CO2_8pct\n",
    "# - High_Temp_38C\n",
    "# - Low_Light_15uE, High_Light\n",
    "\n",
    "RUN_DESEQ2_GROUP2 = False  # Set to True to run\n",
    "\n",
    "if RUN_DESEQ2_GROUP2:\n",
    "    print(\"Running DESeq2 for Group 2 (Environmental)...\\n\")\n",
    "    \n",
    "    # Filter metadata for Group 2\n",
    "    group2_meta = metadata[metadata['group'] == 2].copy()\n",
    "    \n",
    "    # Save metadata for R\n",
    "    group2_meta_file = DIRS['deseq2'] / 'group2_metadata.csv'\n",
    "    group2_meta.to_csv(group2_meta_file, index=False)\n",
    "    \n",
    "    # Run DESeq2 in R\n",
    "    ro.r(f'''\n",
    "    library(DESeq2)\n",
    "    \n",
    "    # Load metadata\n",
    "    coldata <- read.csv(\"{group2_meta_file}\")\n",
    "    rownames(coldata) <- coldata$sample_id\n",
    "    coldata$condition <- factor(coldata$condition)\n",
    "    coldata$condition <- relevel(coldata$condition, ref=\"Control_MAD\")\n",
    "    \n",
    "    # Load counts (subset to Group 2 samples)\n",
    "    counts <- read.csv(\"{DIRS['deseq2']}/counts_matrix.csv\", row.names=1)\n",
    "    counts <- counts[, coldata$sample_id]\n",
    "    counts <- round(counts)\n",
    "    \n",
    "    # Create DESeq2 object\n",
    "    dds <- DESeqDataSetFromMatrix(\n",
    "        countData = counts,\n",
    "        colData = coldata,\n",
    "        design = ~ condition\n",
    "    )\n",
    "    \n",
    "    # Filter low counts\n",
    "    keep <- rowSums(counts(dds) >= 10) >= 3\n",
    "    dds <- dds[keep,]\n",
    "    \n",
    "    # Run DESeq2\n",
    "    dds <- DESeq(dds)\n",
    "    \n",
    "    # Save normalized counts\n",
    "    norm_counts <- as.data.frame(counts(dds, normalized=TRUE))\n",
    "    write.csv(norm_counts, \"{DIRS['deseq2']}/group2_normalized_counts.csv\")\n",
    "    \n",
    "    # Extract results for each comparison\n",
    "    conditions <- levels(coldata$condition)\n",
    "    conditions <- conditions[conditions != \"Control_MAD\"]\n",
    "    \n",
    "    for (cond in conditions) {{\n",
    "        res <- results(dds, contrast=c(\"condition\", cond, \"Control_MAD\"))\n",
    "        res <- as.data.frame(res)\n",
    "        res$gene_id <- rownames(res)\n",
    "        write.csv(res, paste0(\"{DIRS['deseq2']}/group2_\", cond, \"_vs_Control.csv\"), row.names=FALSE)\n",
    "        \n",
    "        sig <- sum(res$padj < 0.05 & abs(res$log2FoldChange) > 1, na.rm=TRUE)\n",
    "        cat(paste0(cond, \" vs Control: \", sig, \" DE genes\\n\"))\n",
    "    }}\n",
    "    ''')\n",
    "    \n",
    "    print(\"\\nGroup 2 analysis complete!\")\n",
    "else:\n",
    "    print(\"Skipping DESeq2 Group 2 (set RUN_DESEQ2_GROUP2 = True to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deseq2-group3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DESeq2 Analysis - Group 3 (Circadian) ===\n",
    "#\n",
    "# Time-series analysis using DESeq2 LRT (Likelihood Ratio Test)\n",
    "# Tests for genes with significant changes across timepoints.\n",
    "#\n",
    "# Timepoints:\n",
    "# - T1 (Light), T2 (Dark), T3 (Light), T4 (Dark)\n",
    "\n",
    "RUN_DESEQ2_GROUP3 = False  # Set to True to run\n",
    "\n",
    "if RUN_DESEQ2_GROUP3:\n",
    "    print(\"Running DESeq2 for Group 3 (Circadian - LRT)...\\n\")\n",
    "    \n",
    "    # Filter metadata for Group 3\n",
    "    group3_meta = metadata[metadata['group'] == 3].copy()\n",
    "    \n",
    "    # Add timepoint numeric for trend analysis\n",
    "    timepoint_map = {'T1_Light': 1, 'T2_Dark': 2, 'T3_Light': 3, 'T4_Dark': 4}\n",
    "    group3_meta['timepoint'] = group3_meta['condition'].map(timepoint_map)\n",
    "    \n",
    "    # Save metadata for R\n",
    "    group3_meta_file = DIRS['deseq2'] / 'group3_metadata.csv'\n",
    "    group3_meta.to_csv(group3_meta_file, index=False)\n",
    "    \n",
    "    # Run DESeq2 LRT in R\n",
    "    ro.r(f'''\n",
    "    library(DESeq2)\n",
    "    \n",
    "    # Load metadata\n",
    "    coldata <- read.csv(\"{group3_meta_file}\")\n",
    "    rownames(coldata) <- coldata$sample_id\n",
    "    coldata$condition <- factor(coldata$condition, \n",
    "                                 levels=c(\"T1_Light\", \"T2_Dark\", \"T3_Light\", \"T4_Dark\"))\n",
    "    \n",
    "    # Load counts\n",
    "    counts <- read.csv(\"{DIRS['deseq2']}/counts_matrix.csv\", row.names=1)\n",
    "    counts <- counts[, coldata$sample_id]\n",
    "    counts <- round(counts)\n",
    "    \n",
    "    # Create DESeq2 object\n",
    "    dds <- DESeqDataSetFromMatrix(\n",
    "        countData = counts,\n",
    "        colData = coldata,\n",
    "        design = ~ condition\n",
    "    )\n",
    "    \n",
    "    # Filter low counts\n",
    "    keep <- rowSums(counts(dds) >= 10) >= 3\n",
    "    dds <- dds[keep,]\n",
    "    \n",
    "    # Run DESeq2 with LRT (tests for ANY difference across timepoints)\n",
    "    dds <- DESeq(dds, test=\"LRT\", reduced=~1)\n",
    "    \n",
    "    # Save normalized counts\n",
    "    norm_counts <- as.data.frame(counts(dds, normalized=TRUE))\n",
    "    write.csv(norm_counts, \"{DIRS['deseq2']}/group3_normalized_counts.csv\")\n",
    "    \n",
    "    # LRT results (genes changing over time)\n",
    "    res_lrt <- results(dds)\n",
    "    res_lrt <- as.data.frame(res_lrt)\n",
    "    res_lrt$gene_id <- rownames(res_lrt)\n",
    "    write.csv(res_lrt, \"{DIRS['deseq2']}/group3_circadian_LRT.csv\", row.names=FALSE)\n",
    "    \n",
    "    sig <- sum(res_lrt$padj < 0.05, na.rm=TRUE)\n",
    "    cat(paste0(\"Genes with significant circadian variation: \", sig, \"\\n\"))\n",
    "    \n",
    "    # Also do pairwise comparisons\n",
    "    # T2 (Dark) vs T1 (Light)\n",
    "    dds_wald <- DESeq(dds, test=\"Wald\")\n",
    "    \n",
    "    res_t2t1 <- results(dds_wald, contrast=c(\"condition\", \"T2_Dark\", \"T1_Light\"))\n",
    "    write.csv(as.data.frame(res_t2t1), \"{DIRS['deseq2']}/group3_T2_Dark_vs_T1_Light.csv\")\n",
    "    \n",
    "    res_t3t2 <- results(dds_wald, contrast=c(\"condition\", \"T3_Light\", \"T2_Dark\"))\n",
    "    write.csv(as.data.frame(res_t3t2), \"{DIRS['deseq2']}/group3_T3_Light_vs_T2_Dark.csv\")\n",
    "    \n",
    "    res_t4t3 <- results(dds_wald, contrast=c(\"condition\", \"T4_Dark\", \"T3_Light\"))\n",
    "    write.csv(as.data.frame(res_t4t3), \"{DIRS['deseq2']}/group3_T4_Dark_vs_T3_Light.csv\")\n",
    "    ''')\n",
    "    \n",
    "    print(\"\\nGroup 3 analysis complete!\")\n",
    "else:\n",
    "    print(\"Skipping DESeq2 Group 3 (set RUN_DESEQ2_GROUP3 = True to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: Visualization\n",
    "\n",
    "### 4.1 QC Plots (PCA, Sample Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PCA Plot ===\n",
    "#\n",
    "# Visualize sample clustering using principal component analysis.\n",
    "\n",
    "PLOT_PCA = False  # Set to True after counts are loaded\n",
    "\n",
    "if PLOT_PCA:\n",
    "    # Load normalized counts\n",
    "    norm_counts = pd.read_csv(DIRS['deseq2'] / 'counts_matrix.csv', index_col=0)\n",
    "    \n",
    "    # Log transform\n",
    "    log_counts = np.log2(norm_counts + 1)\n",
    "    \n",
    "    # PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(log_counts.T)  # Samples as rows\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    pca_df = pd.DataFrame({\n",
    "        'PC1': pca_result[:, 0],\n",
    "        'PC2': pca_result[:, 1],\n",
    "        'sample_id': norm_counts.columns\n",
    "    })\n",
    "    pca_df = pca_df.merge(metadata, on='sample_id')\n",
    "    \n",
    "    # Plot with Plotly\n",
    "    fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2',\n",
    "        color='condition',\n",
    "        symbol='group',\n",
    "        hover_data=['sample_id'],\n",
    "        title='PCA: All Samples',\n",
    "        labels={\n",
    "            'PC1': f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "            'PC2': f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)'\n",
    "        }\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=10))\n",
    "    fig.update_layout(height=600, width=900)\n",
    "    fig.show()\n",
    "    \n",
    "    # Save\n",
    "    fig.write_html(DIRS['figures'] / 'qc_plots' / 'pca_all_samples.html')\n",
    "    print(f\"\\nSaved: {DIRS['figures']}/qc_plots/pca_all_samples.html\")\n",
    "else:\n",
    "    print(\"Skipping PCA plot (set PLOT_PCA = True after loading counts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sample Correlation Heatmap ===\n",
    "\n",
    "PLOT_CORRELATION = False  # Set to True after counts are loaded\n",
    "\n",
    "if PLOT_CORRELATION:\n",
    "    # Load normalized counts\n",
    "    norm_counts = pd.read_csv(DIRS['deseq2'] / 'counts_matrix.csv', index_col=0)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    log_counts = np.log2(norm_counts + 1)\n",
    "    corr_matrix = log_counts.corr(method='pearson')\n",
    "    \n",
    "    # Create annotation colors\n",
    "    sample_order = metadata['sample_id'].tolist()\n",
    "    corr_matrix = corr_matrix.loc[sample_order, sample_order]\n",
    "    \n",
    "    # Plot\n",
    "    fig = px.imshow(\n",
    "        corr_matrix,\n",
    "        labels=dict(color=\"Pearson r\"),\n",
    "        x=corr_matrix.columns,\n",
    "        y=corr_matrix.index,\n",
    "        color_continuous_scale='RdBu_r',\n",
    "        zmin=0.8, zmax=1.0,\n",
    "        title='Sample Correlation Matrix'\n",
    "    )\n",
    "    fig.update_layout(height=800, width=900)\n",
    "    fig.show()\n",
    "    \n",
    "    # Save\n",
    "    fig.write_html(DIRS['figures'] / 'qc_plots' / 'correlation_heatmap.html')\n",
    "else:\n",
    "    print(\"Skipping correlation heatmap (set PLOT_CORRELATION = True after loading counts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "volcano-header",
   "metadata": {},
   "source": [
    "### 4.2 Volcano Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "volcano-plot-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Volcano Plot Function ===\n",
    "\n",
    "def create_volcano_plot(results_file, title, output_path=None, \n",
    "                        log2fc_threshold=1, padj_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Create an interactive volcano plot from DESeq2 results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_file : str or Path\n",
    "        Path to DESeq2 results CSV\n",
    "    title : str\n",
    "        Plot title\n",
    "    output_path : Path, optional\n",
    "        Path to save HTML output\n",
    "    log2fc_threshold : float\n",
    "        Log2 fold change threshold for significance\n",
    "    padj_threshold : float\n",
    "        Adjusted p-value threshold\n",
    "    \"\"\"\n",
    "    # Load results\n",
    "    df = pd.read_csv(results_file)\n",
    "    \n",
    "    # Remove NA values\n",
    "    df = df.dropna(subset=['log2FoldChange', 'padj'])\n",
    "    \n",
    "    # Add -log10(padj)\n",
    "    df['neg_log10_padj'] = -np.log10(df['padj'])\n",
    "    \n",
    "    # Classify genes\n",
    "    conditions = [\n",
    "        (df['padj'] < padj_threshold) & (df['log2FoldChange'] > log2fc_threshold),\n",
    "        (df['padj'] < padj_threshold) & (df['log2FoldChange'] < -log2fc_threshold),\n",
    "    ]\n",
    "    choices = ['Up', 'Down']\n",
    "    df['regulation'] = np.select(conditions, choices, default='NS')\n",
    "    \n",
    "    # Count DE genes\n",
    "    n_up = (df['regulation'] == 'Up').sum()\n",
    "    n_down = (df['regulation'] == 'Down').sum()\n",
    "    \n",
    "    # Color mapping\n",
    "    color_map = {'Up': 'red', 'Down': 'blue', 'NS': 'gray'}\n",
    "    \n",
    "    # Create plot\n",
    "    fig = px.scatter(\n",
    "        df, x='log2FoldChange', y='neg_log10_padj',\n",
    "        color='regulation',\n",
    "        color_discrete_map=color_map,\n",
    "        hover_data=['gene_id', 'baseMean', 'padj'],\n",
    "        title=f\"{title}<br><sub>Up: {n_up}, Down: {n_down}</sub>\",\n",
    "        labels={\n",
    "            'log2FoldChange': 'Log2 Fold Change',\n",
    "            'neg_log10_padj': '-Log10(adjusted p-value)'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add threshold lines\n",
    "    fig.add_hline(y=-np.log10(padj_threshold), line_dash=\"dash\", line_color=\"gray\")\n",
    "    fig.add_vline(x=log2fc_threshold, line_dash=\"dash\", line_color=\"gray\")\n",
    "    fig.add_vline(x=-log2fc_threshold, line_dash=\"dash\", line_color=\"gray\")\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=5, opacity=0.6))\n",
    "    fig.update_layout(height=600, width=800)\n",
    "    \n",
    "    if output_path:\n",
    "        fig.write_html(output_path)\n",
    "        print(f\"Saved: {output_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"Volcano plot function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "volcano-plots-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate Volcano Plots for All Comparisons ===\n",
    "\n",
    "PLOT_VOLCANOS = False  # Set to True after DESeq2 is complete\n",
    "\n",
    "if PLOT_VOLCANOS:\n",
    "    # Find all results files\n",
    "    results_files = list(DIRS['deseq2'].glob('group*_*_vs_*.csv'))\n",
    "    results_files += list(DIRS['deseq2'].glob('group*_circadian_LRT.csv'))\n",
    "    \n",
    "    print(f\"Found {len(results_files)} results files\\n\")\n",
    "    \n",
    "    for results_file in results_files:\n",
    "        # Extract comparison name from filename\n",
    "        name = results_file.stem\n",
    "        title = name.replace('_', ' ').replace('vs', 'vs.')\n",
    "        \n",
    "        output_path = DIRS['figures'] / 'volcano_plots' / f\"{name}.html\"\n",
    "        \n",
    "        fig = create_volcano_plot(\n",
    "            results_file, \n",
    "            title=title,\n",
    "            output_path=output_path\n",
    "        )\n",
    "        # Display first one as example\n",
    "        if results_file == results_files[0]:\n",
    "            fig.show()\n",
    "    \n",
    "    print(f\"\\nAll volcano plots saved to: {DIRS['figures']}/volcano_plots/\")\n",
    "else:\n",
    "    print(\"Skipping volcano plots (set PLOT_VOLCANOS = True after DESeq2 is complete)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap-header",
   "metadata": {},
   "source": [
    "### 4.3 Expression Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Heatmap Function ===\n",
    "\n",
    "def create_expression_heatmap(counts_df, gene_list, sample_ids, metadata, \n",
    "                               title=\"Expression Heatmap\", output_path=None):\n",
    "    \"\"\"\n",
    "    Create a clustered heatmap of gene expression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    counts_df : pd.DataFrame\n",
    "        Normalized counts matrix (genes × samples)\n",
    "    gene_list : list\n",
    "        Genes to include\n",
    "    sample_ids : list\n",
    "        Samples to include\n",
    "    metadata : pd.DataFrame\n",
    "        Sample metadata\n",
    "    title : str\n",
    "        Plot title\n",
    "    output_path : Path, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    # Subset data\n",
    "    data = counts_df.loc[gene_list, sample_ids]\n",
    "    \n",
    "    # Log transform and z-score\n",
    "    log_data = np.log2(data + 1)\n",
    "    z_data = (log_data.T - log_data.mean(axis=1)) / log_data.std(axis=1)\n",
    "    z_data = z_data.T\n",
    "    \n",
    "    # Get condition labels\n",
    "    sample_meta = metadata[metadata['sample_id'].isin(sample_ids)].set_index('sample_id')\n",
    "    conditions = sample_meta.loc[sample_ids, 'condition'].tolist()\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = px.imshow(\n",
    "        z_data,\n",
    "        labels=dict(x=\"Sample\", y=\"Gene\", color=\"Z-score\"),\n",
    "        x=sample_ids,\n",
    "        y=gene_list,\n",
    "        color_continuous_scale='RdBu_r',\n",
    "        zmin=-2, zmax=2,\n",
    "        title=title,\n",
    "        aspect='auto'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=max(400, len(gene_list) * 15), width=900)\n",
    "    \n",
    "    if output_path:\n",
    "        fig.write_html(output_path)\n",
    "        print(f\"Saved: {output_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"Heatmap function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap-top-de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Top DE Genes Heatmap ===\n",
    "\n",
    "PLOT_HEATMAPS = False  # Set to True after DESeq2 is complete\n",
    "\n",
    "if PLOT_HEATMAPS:\n",
    "    # Load normalized counts\n",
    "    norm_counts = pd.read_csv(DIRS['deseq2'] / 'counts_matrix.csv', index_col=0)\n",
    "    \n",
    "    # Example: Top DE genes from Group 2 High_NaCl comparison\n",
    "    results_file = DIRS['deseq2'] / 'group2_High_NaCl_9pct_vs_Control.csv'\n",
    "    \n",
    "    if results_file.exists():\n",
    "        results = pd.read_csv(results_file)\n",
    "        \n",
    "        # Get top 50 DE genes by adjusted p-value\n",
    "        de_genes = results[\n",
    "            (results['padj'] < 0.05) & \n",
    "            (abs(results['log2FoldChange']) > 1)\n",
    "        ].nsmallest(50, 'padj')['gene_id'].tolist()\n",
    "        \n",
    "        # Filter to genes in counts matrix\n",
    "        de_genes = [g for g in de_genes if g in norm_counts.index]\n",
    "        \n",
    "        # Get Group 2 samples\n",
    "        group2_samples = metadata[metadata['group'] == 2]['sample_id'].tolist()\n",
    "        \n",
    "        if de_genes:\n",
    "            fig = create_expression_heatmap(\n",
    "                norm_counts, de_genes, group2_samples, metadata,\n",
    "                title=f\"Top {len(de_genes)} DE Genes: High NaCl vs Control\",\n",
    "                output_path=DIRS['figures'] / 'heatmaps' / 'top_de_high_nacl.html'\n",
    "            )\n",
    "            fig.show()\n",
    "        else:\n",
    "            print(\"No significant DE genes found.\")\n",
    "    else:\n",
    "        print(f\"Results file not found: {results_file}\")\n",
    "else:\n",
    "    print(\"Skipping heatmaps (set PLOT_HEATMAPS = True after DESeq2 is complete)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pipeline Summary ===\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RNA-seq Pipeline Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check what outputs exist\n",
    "def count_files(directory, pattern):\n",
    "    \"\"\"Count files matching pattern in directory.\"\"\"\n",
    "    if not directory.exists():\n",
    "        return 0\n",
    "    return len(list(directory.glob(pattern)))\n",
    "\n",
    "print(\"\\n[Data Inputs]\")\n",
    "print(f\"  Raw FASTQ files: {count_files(DIRS['data'], '*.fastq.gz')}\")\n",
    "\n",
    "print(\"\\n[Pre-processing]\")\n",
    "print(f\"  FastQC raw reports: {count_files(DIRS['qc'] / 'fastqc_raw', '*_fastqc.html')}\")\n",
    "print(f\"  Trimmed files: {count_files(DIRS['trimmed'], '*.fastq.gz')}\")\n",
    "print(f\"  Merged files: {count_files(DIRS['merged'], '*.fastq.gz')}\")\n",
    "print(f\"  rRNA-filtered files: {count_files(DIRS['rrna_filtered'] / 'non_rRNA', '*.fq.gz')}\")\n",
    "\n",
    "print(\"\\n[Quantification]\")\n",
    "print(f\"  Salmon quant directories: {count_files(DIRS['salmon'] / 'quants', '*')}\")\n",
    "\n",
    "print(\"\\n[Alignment]\")\n",
    "print(f\"  BAM files: {count_files(DIRS['alignment'] / 'bam', '*.bam')}\")\n",
    "print(f\"  bigWig files: {count_files(DIRS['alignment'] / 'bigwig', '*.bw')}\")\n",
    "\n",
    "print(\"\\n[DE Analysis]\")\n",
    "print(f\"  Results files: {count_files(DIRS['deseq2'], '*.csv')}\")\n",
    "\n",
    "print(\"\\n[Figures]\")\n",
    "print(f\"  QC plots: {count_files(DIRS['figures'] / 'qc_plots', '*.html')}\")\n",
    "print(f\"  Volcano plots: {count_files(DIRS['figures'] / 'volcano_plots', '*.html')}\")\n",
    "print(f\"  Heatmaps: {count_files(DIRS['figures'] / 'heatmaps', '*.html')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### To run the full pipeline:\n",
    "\n",
    "1. **Activate conda environment**:\n",
    "   ```bash\n",
    "   conda activate pcc11901_rnaseq\n",
    "   ```\n",
    "\n",
    "2. **Set the RUN_* flags to True** for each step you want to execute\n",
    "\n",
    "3. **Execute cells in order** (or run shell scripts separately for compute-intensive steps)\n",
    "\n",
    "### Recommended execution order:\n",
    "\n",
    "| Step | Flag | Notes |\n",
    "|------|------|-------|\n",
    "| FastQC raw | `RUN_FASTQC_RAW` | ~30 min for 240 files |\n",
    "| fastp trimming | `RUN_FASTP` | ~2-3 hours |\n",
    "| Lane merging | `RUN_MERGE` | Fast (~10 min) |\n",
    "| SortMeRNA | `RUN_SORTMERNA` | SLOW (~1 hour/sample, memory-intensive) |\n",
    "| FastQC trimmed | `RUN_FASTQC_TRIMMED` | ~15 min |\n",
    "| MultiQC | `RUN_MULTIQC` | Fast |\n",
    "| gffread | `RUN_GFFREAD` | Fast |\n",
    "| Salmon index | `RUN_SALMON_INDEX` | ~5-10 min |\n",
    "| Salmon quant | `RUN_SALMON_QUANT` | ~30 min total |\n",
    "| Bowtie2 index | `RUN_BOWTIE2_INDEX` | ~5 min |\n",
    "| Bowtie2 align | `RUN_BOWTIE2_ALIGN` | ~2-3 hours |\n",
    "| bigWig | `RUN_BIGWIG` | ~30 min |\n",
    "| Load counts | `LOAD_COUNTS` | Fast |\n",
    "| DESeq2 Group 1 | `RUN_DESEQ2_GROUP1` | Fast |\n",
    "| DESeq2 Group 2 | `RUN_DESEQ2_GROUP2` | Fast |\n",
    "| DESeq2 Group 3 | `RUN_DESEQ2_GROUP3` | Fast |\n",
    "| Visualization | `PLOT_*` | Fast |\n",
    "\n",
    "### Future analyses (not implemented):\n",
    "\n",
    "- Functional enrichment (GO, KEGG) via eggNOG-mapper + clusterProfiler\n",
    "- Gene set enrichment analysis (GSEA)\n",
    "- Co-expression network analysis\n",
    "- Pathway visualization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}